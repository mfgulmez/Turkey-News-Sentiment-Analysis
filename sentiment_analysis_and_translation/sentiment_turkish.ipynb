{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "166a48b3-606e-41dc-820e-fe070646351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.auto import tqdm  # For progress visualization\n",
    "from math import ceil\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ad4d895-21bf-4cd2-817f-00e885d3ecab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS (Metal Performance Shaders) is available. Using GPU for inference.\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    \"\"\"\n",
    "    Detects and returns the available device for PyTorch computations.\n",
    "    Priority: MPS (for Apple Silicon) > CUDA > CPU\n",
    "    \"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"MPS (Metal Performance Shaders) is available. Using GPU for inference.\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"CUDA is available. Using GPU for inference.\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"No GPU available. Using CPU for inference.\")\n",
    "    return device\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5429150-b4f9-4f4f-9382-e661bf9324e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Number of rows: 1000\n",
      "DataFrame columns: ['source', 'article_url', 'title', 'date', 'shortened_full_text', 'combined_text', 'sentiment_label', 'sentiment_score', 'title_tr', 'shortened_full_text_tr', 'title_and_text', 'normalized_text', 'entities', 'keywords']\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = \"./processed_news_turkish_entities_keywords.csv\"\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if not os.path.exists(csv_file_path):\n",
    "    raise FileNotFoundError(f\"CSV file not found at path: {csv_file_path}\")\n",
    "\n",
    "# Load the CSV into a pandas DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Verify the presence of the 'normalized_text' column\n",
    "if 'normalized_text' not in df.columns:\n",
    "    raise ValueError(\"The 'normalized_text' column does not exist in the provided CSV.\")\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(f\"Number of rows: {len(df)}\")\n",
    "print(\"DataFrame columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "054bd171-5fbf-46fc-a796-b99bf64671d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"savasy/bert-base-turkish-sentiment-cased\"\n",
    "\n",
    "try:\n",
    "    # Load the tokenizer with truncation settings\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    \n",
    "    # Load the pre-trained model for sequence classification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    \n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model '{model_name}': {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26ee9357-e46f-484d-9325-8b46947bd737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis pipeline initialized with truncation.\n"
     ]
    }
   ],
   "source": [
    "# Determine the device index for the pipeline\n",
    "if device.type == \"cuda\":\n",
    "    device_index = 0  # Assuming using the first CUDA device\n",
    "elif device.type == \"mps\":\n",
    "    device_index = -1  # Transformers library uses -1 for MPS\n",
    "else:\n",
    "    device_index = -1  # CPU\n",
    "\n",
    "# Initialize the sentiment analysis pipeline with truncation\n",
    "sentiment_analyzer = pipeline(\n",
    "    task=\"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device_index,\n",
    "    truncation=True,       \n",
    "    max_length=512            \n",
    ")\n",
    "\n",
    "print(\"Sentiment analysis pipeline initialized with truncation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2d29bc5-a077-481f-90d9-531ef3ff55d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, tokenizer, max_length=512, stride=50):\n",
    "    \"\"\"\n",
    "    Splits the input text into chunks of tokens not exceeding max_length.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to split.\n",
    "        tokenizer (transformers.Tokenizer): The tokenizer used to encode the text.\n",
    "        max_length (int): The maximum number of tokens per chunk.\n",
    "        stride (int): The number of tokens to overlap between chunks.\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    # Encode the text to get token IDs\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        return_attention_mask=False,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    input_ids = encoded['input_ids']\n",
    "    \n",
    "    # Calculate the number of chunks\n",
    "    total_length = len(input_ids)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < total_length:\n",
    "        end = start + max_length\n",
    "        chunk_ids = input_ids[start:end]\n",
    "        # Decode back to text\n",
    "        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "        if end >= total_length:\n",
    "            break\n",
    "        start += max_length - stride  # Move by (max_length - stride) tokens\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f9b9793-95f7-4748-8041-0ecee3722570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c23f41a287043c08ea4af8c5af7452a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Performing sentiment analysis:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed. Sentiment columns added to the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Define new columns for predicted sentiment and scores\n",
    "predicted_label_column = \"turkish_sentiment_label\"\n",
    "predicted_score_column = \"turkish_sentiment_score\"\n",
    "\n",
    "# Initialize lists to store predictions\n",
    "predicted_labels = []\n",
    "predicted_scores = []\n",
    "\n",
    "# Configure logging to capture errors\n",
    "logging.basicConfig(filename='inference_errors.log', level=logging.ERROR)\n",
    "\n",
    "# Iterate over each text entry\n",
    "for text in tqdm(df[\"normalized_text\"], desc=\"Performing sentiment analysis\"):\n",
    "    # Handle missing or non-string text\n",
    "    if not isinstance(text, str):\n",
    "        text = \"\"\n",
    "    \n",
    "    # Split text into chunks if necessary\n",
    "    tokenized = tokenizer.encode(text, add_special_tokens=True)\n",
    "    if len(tokenized) > 512:\n",
    "        chunks = split_text(text, tokenizer, max_length=512, stride=50)\n",
    "    else:\n",
    "        chunks = [text]\n",
    "    \n",
    "    chunk_labels = []\n",
    "    chunk_scores = []\n",
    "    \n",
    "    # Analyze each chunk\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            result = sentiment_analyzer(chunk)[0]\n",
    "            # Example result: {'label': 'POSITIVE', 'score': 0.998}\n",
    "            chunk_labels.append(result[\"label\"])\n",
    "            chunk_scores.append(result[\"score\"])\n",
    "        except Exception as e:\n",
    "            # Log the error and append default values\n",
    "            logging.error(f\"Error processing chunk: {e}\")\n",
    "            chunk_labels.append(\"UNKNOWN\")\n",
    "            chunk_scores.append(0.0)\n",
    "    \n",
    "    # Aggregate results for the entire text\n",
    "    if chunk_scores:\n",
    "        # Example aggregation: Majority vote for labels, average for scores\n",
    "        label_counts = {}\n",
    "        for label in chunk_labels:\n",
    "            label_counts[label] = label_counts.get(label, 0) + 1\n",
    "        aggregated_label = max(label_counts, key=label_counts.get)\n",
    "        aggregated_score = sum(chunk_scores) / len(chunk_scores)\n",
    "    else:\n",
    "        aggregated_label = \"UNKNOWN\"\n",
    "        aggregated_score = 0.0\n",
    "    \n",
    "    # Append aggregated results to the lists\n",
    "    predicted_labels.append(aggregated_label)\n",
    "    predicted_scores.append(aggregated_score)\n",
    "\n",
    "# Add the prediction results to the DataFrame\n",
    "df[predicted_label_column] = predicted_labels\n",
    "df[predicted_score_column] = predicted_scores\n",
    "\n",
    "print(\"Inference completed. Sentiment columns added to the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5e4ea56-c4ae-4c42-9d03-cfa203897011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Distribution:\n",
      "turkish_sentiment_label\n",
      "negative    560\n",
      "positive    440\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample Predictions:\n",
      "  turkish_sentiment_label  turkish_sentiment_score\n",
      "0                positive                 0.979294\n",
      "1                positive                 0.721456\n",
      "2                negative                 0.717170\n",
      "3                positive                 0.991640\n",
      "4                positive                 0.735413\n"
     ]
    }
   ],
   "source": [
    "# Display the distribution of predicted sentiment labels\n",
    "print(\"Label Distribution:\")\n",
    "print(df[predicted_label_column].value_counts())\n",
    "\n",
    "# Optional: Display sample predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(df[[predicted_label_column, predicted_score_column]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c7d70d6-ba6b-48e6-a5d4-f7be20408f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved to 'updated_data_with_turkish_sentiment.csv' with new sentiment columns.\n"
     ]
    }
   ],
   "source": [
    "# Specify the output CSV file path\n",
    "output_csv_file_path = \"updated_data_with_turkish_sentiment.csv\"  # Replace with desired output filename\n",
    "\n",
    "# Save the DataFrame to CSV\n",
    "df.to_csv(output_csv_file_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV saved to '{output_csv_file_path}' with new sentiment columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28a1619-dd3c-4e9d-8f74-006ce1b21d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sentiment_env_v35)",
   "language": "python",
   "name": "sentiment_env_v35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
